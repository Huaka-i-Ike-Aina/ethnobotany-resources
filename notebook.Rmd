---
output: github_document
---


```{r, include = FALSE}

packages <- c("rvest", "xml2", "dplyr", "purrr", "stringr"
              "rmarkdown", "knitr")

install_if_missing <- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

invisible(lapply(packages, install_if_missing))


knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```
	
# Scraping BM-EB

```{r scrape-bmeb}

# --- Install required packages only if missing ---


# --- Load libraries ---
library(rvest)
library(purrr)
library(dplyr)
library(stringr)
library(rvest)
library(xml2)
library(dplyr)

# --- Define the URL ---
url <- "https://data.bishopmuseum.org/ethnobotanydb/ethnobotany.php?b=list&o=1"

# --- Read the HTML content ---
page <- read_html(url)

# --- Extract all <table> elements ---
tables <- page %>% html_nodes("table")

# --- Check how many tables are found ---
length(tables)

# --- Convert the first table to a data frame ---
ethno_table <- tables[[1]] %>% html_table(fill = TRUE)

# --- Preview the data ---
glimpse(ethno_table)

# Optional: View first rows
head(ethno_table)

# --- Create 'data' directory if it doesn't exist ---
if (!dir.exists("data")) {
  dir.create("data")
}

# --- Save the table as CSV in the 'data' directory ---
write.csv(ethno_table, file = "data/ethno_table.csv", row.names = FALSE)


```


# Display BM EB Table

```{r bmeb-table}

library(knitr)

bmeb <- read.csv("data/ethno_table.csv")

kable(bmeb, caption = "Ethnobotany plants in Bishop Museum")


```

# Webcrawler

```{r crawl}

# Install/load packages
pkgs <- c("rvest", "data.table", "purrr", "stringr")
sapply(pkgs, function(p) if (!requireNamespace(p, quietly = TRUE)) install.packages(p))
library(rvest)
library(data.table)
library(purrr)
library(stringr)

                                        # Base URL (note: page numbers go in o=)
base_url_template <- "https://data.bishopmuseum.org/ethnobotanydb/ethnobotany.php?b=list&o=%d"

                                        # ---- Step 1: Function to scrape one depth-1 page ----
scrape_depth1_page <- function(page_number) {
    url <- sprintf(base_url_template, page_number)
    message("Scraping depth-1 page: ", url)

    page <- read_html(url)

                                        # Check if there is a table; return NULL if none
    tbl_node <- page %>% html_node("table")
    if (is.na(tbl_node) || length(tbl_node) == 0) return(NULL)

    main_table <- tbl_node %>% html_table(fill = TRUE)
    if (nrow(main_table) == 0) return(NULL)

    dt <- as.data.table(main_table)
    setnames(dt, make.names(names(dt), unique = TRUE))

                                        # Get Hawaiian Name links
    hawaiian_nodes <- page %>%
        html_nodes("table tr td:nth-child(1) a")

    hawaiian_links <- hawaiian_nodes %>%
        html_attr("href") %>%
        url_absolute(url)

    dt[, Detail_URL := hawaiian_links]

    return(dt)
}

                                        # ---- Step 2: Function to scrape depth-2 details ----
scrape_detail_page <- function(url) {
    page <- read_html(url)

    detail_tables <- page %>%
        html_nodes("table") %>%
        html_table(fill = TRUE)

    if (length(detail_tables) == 0) return(list())

    detail_df <- detail_tables[[1]]

    if (ncol(detail_df) >= 2) {
        labels <- str_trim(detail_df[[1]])
        values <- str_trim(detail_df[[2]])
        names(values) <- make.names(labels, unique = TRUE)
        values <- as.list(values)
    } else {
        values <- list()
    }

    Sys.sleep(runif(1, 1, 3))  # pause between 1â€“3 seconds

    return(values)
}

                                        # ---- Step 3: Loop over all pagination ----
all_depth1 <- list()
page_number <- 1

repeat {
    dt <- scrape_depth1_page(page_number)
    if (is.null(dt)) break

    all_depth1[[length(all_depth1) + 1]] <- dt
    page_number <- page_number + 1
}

depth1_dt <- rbindlist(all_depth1, fill = TRUE)

                                        # ---- Step 4: Scrape all depth-2 pages ----
detail_list <- map(depth1_dt$Detail_URL, scrape_detail_page)

                                        # ---- Step 5: Merge into one normalized data.table ----
details_dt <- rbindlist(detail_list, fill = TRUE)
final_dt <- cbind(depth1_dt, details_dt)

                                        # ---- Step 6: Save to CSV ----
fwrite(final_dt, "data/ethnobotany_full_normalized.csv")

message("Scraping complete! Rows: ", nrow(final_dt), ", Columns: ", ncol(final_dt))

```


```{r crawler-results}


```
