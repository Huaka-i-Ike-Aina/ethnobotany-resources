---
output: github_document
---


```{r, include = FALSE}

packages <- c("rvest", "xml2", "dplyr", "purrr", "stringr"
              "rmarkdown", "knitr")

install_if_missing <- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

invisible(lapply(packages, install_if_missing))


knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```
	
# Scraping BM-EB

```{r scrape-bmeb}

# --- Install required packages only if missing ---


# --- Load libraries ---
library(rvest)
library(purrr)
library(dplyr)
library(stringr)
library(rvest)
library(xml2)
library(dplyr)

# --- Define the URL ---
url <- "https://data.bishopmuseum.org/ethnobotanydb/ethnobotany.php?b=list&o=1"

# --- Read the HTML content ---
page <- read_html(url)

# --- Extract all <table> elements ---
tables <- page %>% html_nodes("table")

# --- Check how many tables are found ---
length(tables)

# --- Convert the first table to a data frame ---
ethno_table <- tables[[1]] %>% html_table(fill = TRUE)

# --- Preview the data ---
glimpse(ethno_table)

# Optional: View first rows
head(ethno_table)

# --- Create 'data' directory if it doesn't exist ---
if (!dir.exists("data")) {
  dir.create("data")
}

# --- Save the table as CSV in the 'data' directory ---
write.csv(ethno_table, file = "data/ethno_table.csv", row.names = FALSE)


```


# Display BM EB Table

```{r bmeb-table}

library(knitr)

bmeb <- read.csv("data/ethno_table.csv")

kable(bmeb, caption = "Ethnobotany plants in Bishop Museum")


```

# Webcrawler

Coded with assistance from ChatGPT. 

Currently should have the following functionality: 

* Scrapes all pages with pagination,
* Pulls all depth-2 details,
* Normalizes them into a single `data.table`,
* Polite throttling included, and
* Can resume if interrupted.

```{r crawl}

# Install/load packages
pkgs <- c("rvest", "data.table", "purrr", "stringr")
sapply(pkgs, function(p) if (!requireNamespace(p, quietly = TRUE)) install.packages(p))
library(rvest)
library(data.table)
library(purrr)
library(stringr)

                                        # Base URL (pagination placeholder)
base_url_template <- "https://data.bishopmuseum.org/ethnobotanydb/ethnobotany.php?b=list&o=%d"

                                        # Output files for resume
depth1_file <- "ethnobotany_depth1_progress.csv"
final_file  <- "ethnobotany_full_normalized.csv"

                                        # ---- Step 1: Load existing depth-1 data if resuming ----
if (file.exists(depth1_file)) {
    depth1_dt <- fread(depth1_file)
    message("Resuming with ", nrow(depth1_dt), " depth-1 rows already scraped.")
} else {
    depth1_dt <- data.table()
}

                                        # ---- Step 2: Scrape depth-1 pages ----
scrape_depth1_page <- function(page_number) {
    url <- sprintf(base_url_template, page_number)
    message("Scraping depth-1 page: ", url)

    page <- tryCatch(read_html(url), error = function(e) return(NULL))
    if (is.null(page)) return(NULL)

    tbl_node <- page %>% html_node("table")
    if (is.na(tbl_node) || length(tbl_node) == 0) return(NULL)

    main_table <- tbl_node %>% html_table(fill = TRUE)
    if (nrow(main_table) == 0) return(NULL)

    dt <- as.data.table(main_table)
    setnames(dt, make.names(names(dt), unique = TRUE))

                                        # Hawaiian Name links
    hawaiian_nodes <- page %>%
        html_nodes("table tr td:nth-child(1) a")

    hawaiian_links <- hawaiian_nodes %>%
        html_attr("href") %>%
        url_absolute(url)

    dt[, Detail_URL := hawaiian_links]

    Sys.sleep(runif(1, 1, 3))  # polite random pause
    return(dt)
}

                                        # If no depth-1 data yet, start scraping from page 1
if (nrow(depth1_dt) == 0) {
    all_depth1 <- list()
    page_number <- 1

    repeat {
        dt <- scrape_depth1_page(page_number)
        if (is.null(dt)) break
        all_depth1[[length(all_depth1) + 1]] <- dt
        depth1_dt <- rbindlist(all_depth1, fill = TRUE)
        fwrite(depth1_dt, depth1_file)  # save progress
        page_number <- page_number + 1
    }
}

                                        # ---- Step 3: Scrape depth-2 detail pages ----
scrape_detail_page <- function(url) {
    message("   Depth-2: ", url)

    page <- tryCatch(read_html(url), error = function(e) return(NULL))
    if (is.null(page)) return(list())

    detail_tables <- page %>%
        html_nodes("table") %>%
        html_table(fill = TRUE)

    if (length(detail_tables) == 0) return(list())

    detail_df <- detail_tables[[1]]

    if (ncol(detail_df) >= 2) {
        labels <- str_trim(detail_df[[1]])
        values <- str_trim(detail_df[[2]])
        names(values) <- make.names(labels, unique = TRUE)
        values <- as.list(values)
    } else {
        values <- list()
    }

    Sys.sleep(runif(1, 1, 3))  # polite random pause
    return(values)
}

                                        # ---- Step 4: Resume-aware depth-2 scraping ----
if (file.exists(final_file)) {
    final_dt <- fread(final_file)
    scraped_urls <- unique(final_dt$Detail_URL)
} else {
    final_dt <- NULL
    scraped_urls <- character()
}

                                        # Filter only URLs we haven't scraped yet
pending_urls <- setdiff(depth1_dt$Detail_URL, scraped_urls)
message("Depth-2 pages remaining: ", length(pending_urls))

                                        # Scrape only pending detail pages
detail_list <- map(pending_urls, scrape_detail_page)

                                        # Convert to data.table
details_dt <- rbindlist(detail_list, fill = TRUE)

                                        # Match them back to depth1 rows
new_data <- depth1_dt[Detail_URL %in% pending_urls]
final_batch <- cbind(new_data, details_dt)

                                        # Append to final dataset
if (!is.null(final_dt)) {
    final_dt <- rbindlist(list(final_dt, final_batch), fill = TRUE)
} else {
    final_dt <- final_batch
}

                                        # Save progress
fwrite(final_dt, final_file)

message("Scraping complete! Rows: ", nrow(final_dt), ", Columns: ", ncol(final_dt))

```


```{r crawler-results}


```
